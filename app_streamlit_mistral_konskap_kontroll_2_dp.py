# app_streamlit_mistral RAG (Lokal) Mistral + Ollama
# OBS:
# Jag b√∂rjade f√∂rst med Google Gemini API (3 nycklar fr√•n olika gmail konton‚Ä¶ ja, jag vet üòÖ).
# Tanken var om API1 tar slut den hoppa till API2 sen API3. I praktiken mina t√§nka var kaos.
# Timeouts, token-limit, och varje g√•ng jag √§ndrade i koden br√∂ts fl√∂det.
# Jag skrev till och med en hybrid (API f√∂rst, annars lokalt), men det blev f√∂r r√∂rigt.
# Slutsats: 100% lokalt med Ollama + Mistral. Mindre ‚Äúmagiskt‚Äù, mer kontroll.
# (Och ja ‚Äì Nomic-embedding f√∂rs√∂kte √§ta 33GB RAM p√• min 16GB-laptop‚Ä¶ )

import os
import subprocess
import numpy as np
import faiss
import fitz  # PyMuPDF (jag testade PyPDF2 f√∂rst men tappade text ibland )
import streamlit as st
from typing import List
from sentence_transformers import SentenceTransformer

# Streamlit , det funkar och g√•r snabbt att visa upp.
st.set_page_config(page_title="RAG (Lokal) - Mistral + Ollama", layout="wide")
st.title(" RAG (Lokal) - Mistral + Ollama")
st.caption("PDF in ‚Üí FAISS-index ‚Üí sammanfattning + fr√•gor/svar. Allt lokalt efter f√∂rsta modellen.")

# ---------------- Sidopanel ----------------
st.sidebar.subheader(" Inst√§llningar (lek med dessa om svaren blir konstiga)")
embed_choice = st.sidebar.selectbox(
    "Inb√§ddningsmodell",
    ["sentence-transformers/all-MiniLM-L6-v2", "nomic-ai/nomic-embed-text-v1.5"],
    index=0  # MiniLM √§r ‚Äúdet funkar alltid‚Äù-l√§get
)
auto_build = st.sidebar.checkbox(" Bygg FAISS-index automatiskt efter uppladdning", value=False)
top_k = st.sidebar.slider("k (antal segment som h√§mtas)", 1, 8, 4)  # 3‚Äì5 brukar vara lagom
chunk_size = st.sidebar.slider("Segmentstorlek (ord)", 300, 2000, 1200, step=100)  # 1200 funkar bra f√∂r boken
overlap = st.sidebar.slider("√ñverlappning (ord)", 0, 400, 200, step=50)  # lite √∂verlapp = mindre info-tapp
max_summary_sents = st.sidebar.slider("Antal meningar i sammanfattning", 2, 8, 3)

st.sidebar.markdown("**Ladda modeller (eng√•ngs):**")
st.sidebar.code("ollama pull mistral\nollama pull nomic-embed-text", language="bash")

# ---------------- Funktioner ----------------
def run_mistral(prompt: str) -> str:
    """
    K√∂r lokal Mistral via Ollama. Yes, subprocess. Kunde k√∂rt HTTP-API ocks√•.
    Om du f√•r tomt svar ‚Üí kolla ollama service, eller att modellen finns lokalt.
    """
    result = subprocess.run(
        ["ollama", "run", "mistral"],
        input=prompt.encode("utf-8"),
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE
    )
    out = result.stdout.decode("utf-8", errors="ignore").strip()
    if not out:
        st.warning(result.stderr.decode("utf-8", errors="ignore"))
    return out

def pdf_bytes_to_text(pdf_bytes: bytes) -> str:
    """
    Extrahera ren text ur PDF. Jag svettades lite h√§r i b√∂rjan.
    PyMuPDF (fitz) funkar b√§st f√∂r mig  guld v√§rt om layouten √§r ‚Äúnormal‚Äù.
    """
    with fitz.open(stream=pdf_bytes, filetype="pdf") as doc:
        parts = []
        for p in doc:
            parts.append(p.get_text("text"))
    return "\n".join(parts)

def chunk_text_words(text: str, chunk_size: int, overlap: int) -> List[str]:
    """
    Delar upp text i bitar (chunks). F√∂r stor chunk = RAM d√∂r (hej Nomic‚Ä¶),
    f√∂r liten = modellen fattar inte sammanhanget. 1200/200 blev ‚Äúlagom‚Äù h√§r.
    """
    words = text.split()
    chunks = []
    i = 0
    step = max(1, chunk_size - overlap)
    while i < len(words):
        chunk = " ".join(words[i:i+chunk_size])
        chunks.append(chunk)
        i += step
    return chunks

@st.cache_resource(show_spinner=False)
def load_embedder(model_name: str) -> SentenceTransformer:
    """
    Laddar inb√§ddningsmodellen. OBS: Nomic kr√§ver trust_remote_code=True.
    Det k√§ndes lite trokigt men annars v√§grar den k√∂ra. MiniLM har inte det kravet.
    """
    if model_name == "nomic-ai/nomic-embed-text-v1.5":
        return SentenceTransformer(model_name, trust_remote_code=True)
    return SentenceTransformer(model_name)

def encode_texts(embedder: SentenceTransformer, texts: List[str], batch_size: int = 32) -> np.ndarray:
    """
    Skapar embeddings. Om det h√§r segar: s√§nk batch_size (t.ex. 16 eller 8).
    Jag f√∂rs√∂ker h√•lla det enkelt h√§r ‚Äì ingen GPU-trollkonst, bara rakt p√•.
    """
    vecs = embedder.encode(
        texts,
        batch_size=batch_size,
        convert_to_numpy=True,
        show_progress_bar=True,
        normalize_embeddings=True,
    )
    return vecs.astype("float32")

def build_faiss_ip(embeddings: np.ndarray) -> faiss.Index:
    """
    Bygger ett FAISS-index (inner product + L2-normalisering).
    Annoy/HNSW √§r fina, men FAISS √§r stabilt och ‚Äújust works‚Äù f√∂r mig.
    """
    dim = embeddings.shape[1]
    idx = faiss.IndexFlatIP(dim)
    faiss.normalize_L2(embeddings)
    idx.add(embeddings)
    return idx

def retrieve(embedder: SentenceTransformer, index: faiss.Index, chunks: List[str], query: str, top_k: int):
    """
    H√§mtar de mest relevanta textbitarna f√∂r fr√•gan.
    Om du byter embedding-modell beh√∂ver du bygga om indexet,
     annars kan det bli fel p√• dimensionerna.

    """
    q = encode_texts(embedder, [query])
    faiss.normalize_L2(q)
    D, I = index.search(q, top_k)
    hits = [chunks[i] for i in I[0]]
    scores = D[0].tolist()
    return hits, scores

def summarize_context(context: str, max_sents: int) -> str:
    """
    Sammanfattar texten kort och bara med fakta.
    Bra n√§r tr√§ffarna √§r l√•nga och vi vill ge modellen en enklare input.
    """
    prompt = (
        f"G√∂r en mycket kort och exakt sammanfattning av texten nedan. "
        f"Max {max_sents} meningar. Endast fakta.\n\n"
        f"Text:\n{context}\n\nSammanfattning:"
    )
    return run_mistral(prompt)

def answer_with_context(question: str, context: str, summary: str = "") -> str:
    """
    Slutliga svaret: fr√•ga + (sammanfattning) + k√§lltext ‚Üí Mistral.
    Om svaret inte finns i texten vill jag hellre att den s√§ger det.
    """
    prompt = (
        "Besvara fr√•gan nedan med hj√§lp av den givna texten. "
        "Om svaret inte finns i texten, skriv 'Finns ej i texten'.\n\n"
        f"Sammanfattning (frivillig): {summary}\n\n"
        f"Textk√§lla:\n{context}\n\n"
        f"Fr√•ga: {question}\nSvar:"
    )
    return run_mistral(prompt)

# ---------------- Huvuddel ----------------
uploaded = st.file_uploader("V√§lj en PDF-fil", type=["pdf"])
if uploaded:
    # H√§r k√§nde jag mig nerv√∂s f√∂rsta g√•ngen ‚Äì stora PDF:er kan ta en stund.
    with st.spinner(" Extraherar text fr√•n PDF ..."):
        full_text = pdf_bytes_to_text(uploaded.read())
    with st.spinner(" Delar upp text i segment ..."):
        chunks = chunk_text_words(full_text, chunk_size=chunk_size, overlap=overlap)
    st.success(f" Antal segment: {len(chunks)} (om detta √§r <50 har du nog f√∂r stor chunk-size)")

    # En enkel cache-nyckel s√• vi inte bygger om hela tiden i on√∂dan.
    cache_key = (hash(full_text), embed_choice, chunk_size, overlap)
    if "stores" not in st.session_state:
        st.session_state["stores"] = {}
    stores = st.session_state["stores"]

    def build_and_cache():
        with st.spinner(" Laddar inb√§ddningsmodell ..."):
            embedder = load_embedder(embed_choice)
        with st.spinner(" Skapar embeddingar och bygger FAISS-index ... "):
            vecs = encode_texts(embedder, chunks)
            index = build_faiss_ip(vecs)
        stores[cache_key] = {"embedder": embedder, "index": index, "chunks": chunks}
        st.success(" FAISS-index byggt. (Sk√∂nt!)")

    col1, col2 = st.columns(2)
    with col1:
        if st.button(" Bygg FAISS-index"):
            build_and_cache()
    with col2:
        if auto_build and cache_key not in stores:
            build_and_cache()

    if cache_key in stores:
        store = stores[cache_key]
        st.divider()
        st.subheader(" Fr√•gor och svar (RAG)")

        q = st.text_input("St√§ll en fr√•ga:", placeholder="Exempel: Vad √§r ett konvolutionslager?")
        if q:
            with st.spinner(" S√∂ker relevanta segment ..."):
                hits, scores = retrieve(store["embedder"], store["index"], store["chunks"], q, top_k=top_k)
                context = "\n\n---\n\n".join(hits)
            with st.spinner(" Sammanfattar text ..."):
                brief = summarize_context(context, max_sents=max_summary_sents)
            with st.spinner(" Genererar svar ..."):
                answer = answer_with_context(q, context, summary=brief)

            st.markdown("###  Relevanta segment")
            for i, (h, s) in enumerate(zip(hits, scores), 1):
                with st.expander(f"Segment {i}  |  score={s:.3f}"):
                    st.write(h)

            st.markdown("###  Sammanfattning")
            st.write(brief)

            st.markdown("###  Svar")
            st.write(answer)
else:
    st.info(" Ladda upp en PDF f√∂r att b√∂rja.")

"""
------------------------------------------------
Kommentarer kring API och lokal l√∂sning (√§rlig version)
------------------------------------------------
Jag anv√§nde Google Gemini API f√∂rst (API1, API2, API3). Det funkade
ibland f√∂rsta k√∂rningen, men sen: timeouts, token-limits  och jag
blev tokig. Jag skrev en fallback (API ‚Üí lokalt), men det blev f√∂r mycket
‚Äútejpa ihop‚Äù-k√§nsla. S√• jag best√§mde mig: bara lokalt med Ollama + Mistral.
√Ñr det perfekt? ja, och jag kan √•tminstone jobba
utan att be en server om lov varje g√•ng.
"""
